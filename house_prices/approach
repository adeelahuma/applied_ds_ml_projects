First Attempt : 
    - In training data handle missing data; filling with median or a certain categoery or 0 for null year 
    - change object types to category types
    - add dummy variables for all the categories
    - remove one dummy for each category ( dummy variable trap)
    - remove the categorical column for which we have created category
    - scale data 
    - Model 
        - split the training data into test/train split 20:80 ratio
        - build ols, lasso, ridge
        - check model prediction on test from split
        - calculate RMSE
    - Check Model output on the given 'test' data   [FAILED] 
        It FAILED because:
            - the test data has 'missing' data for categorical variables 
            - the missing categories are different in train and test file, leading to inconsistent dummy columns in test data
            - Now because number of columns that model is built is different than the number of columns in test data, we can't really use the model as it is
            - We can add additional columns for categories that are missing in the test data BUT the additional categories that are present in test data are missing in the model so its not a good approach/model


Second Attempt: 
    - change object types to category types based on data dictionary in both test and train data. This way 
        - data gets all the potential categories for the column based on data dictionary
        - in categories NA is decoded as -1
        - also made few columns ordered as per data dictionary 
        - made 'year' columns also categorical BUT they need binning  
    - combine training and test data to get a better insight of missing data
    - in training data handled numerical missing data by filling it with median value of the column in train data
    - ISSUE: Assumed that nan is representing 'not applicable' column such as No Alley, No Fence; remove all 'NA', 'No fence' etc from data dictionary so that we don't get two hot-encoded NAN columns
    - After one-hot encoding dropped nan for all categories so model is baselined when category's value is missing
    - ISSUE: We got 610 hot-encoded columns whereas data size is only about 1500 records. Too many columns
    - ISSUE: Number of columns are again different in model and test data
        - columns missing in test data --> all one-hot-encoded columns for which the test data had no values
        - columns missing in model
        - year column is problematic because some years missing in test data and some missing in train data
        - Good solution solution is binning
        - For now removed the column that model is unaware of (additional cols in test data) and add columns that model is aware of (columns lacking in test data) ; Not a good strategy but doing to keep going. 
        
    - Applied the data conversion to categorical data in TEST data as well assuming we can do it. If we don't do it then numerical categories are not properly represnted.     
    - ISSUE: at predict time due to missing values in numerical data. What do i do here? 
        - Can not fill missing values with median in test data   
        - Filled missing values with 0
        - FIRST KAGGLE Submission ; score = 0.24327; position = 4395
        
    - Next TODO:    
        - fill missing numerical value with the median of train data vs median of combined data
        - Binning for year columns because its creating mismatched columns both in model and test data
        - Does NAN in TEST Data means missing value or not applicable ?
        - Does nan in train data represents a missing value or a not applicable value such as 'No Alley'
        - Due to too many columns ; try catboost
        - Hyperparameter tunning for RF : number of trees etc
    
Third Attempt:
    - Exploratory analysis of all columns:
        - Sale Price : boxplot (it is showing outliers, could be expensive houses), distplot, 
        - Categorical columns : catplot, boxplot, regplot
        - Numerical columns:  
            - distplot , mean|median|mode of each column, boxplot
            - saleprice vs other columns : regplot, relplot 
            - year columns 
    - Based on EDA, converted following columns to categories:
        - BsmtFullBath -- Odered 
        - BsmtHalfBath -- (no-negative trend )
        - FullBath -- ordered (upward trend)
        - HalfBath -- ordered (upward trend)
        - BedroomAbvGr -- ordered (upward trend)
        - KitchenAbvGr -- ordered (down trend)
        - TotRmsAbvGrd -- ordered (upward trend)
        - Fireplaces -- ordered (upward trend)
        - GarageCars -- ordered (upward trend)
        - MoSold -- ordered
        - Fix order for ordered categories; the first one in the list is minimum and the last one is max
        - Add ordering for year /month columns   
      - Added Binning by pd.cut() for : 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'MoSold'  
      - Based on EDA, removed following columns: 
            - Not meaningful : BSMTFin2 , LowQualFinSF, EnclosedPorch, 3SsnPorch, ScreenPorch, MiscVal, PoolArea
            - columns for which binning is added
      -  Did all data wrangling (data_dictionary update, column type upfdate, binning , dropping ) on combined data to have consistent data for model and testing. 
      - Filled missing values only for train data based on median of combined data
      - Filled missing values with 0 for test data
      - SECOND KAGGLE Submission ; score = 0.16459; position = 3604 [Issue: model had 700+ columns; forgot to remove binned columns]
      - THIRD KAGGLE Submission ; score = 0.16437; position = 3600 [ model has 396 columns]
            
    - Next TODO
        - should we remove rows where sale price > 400K; considering them as outliers? They could be expensive houses
        - binning equal width binning (cut)?  vs equal distribution binning qcut ?
        - Regularization
        - write a strory about what matters most for house prices 
        - Due to too many columns ; try catboost
        - Hyperparameter tunning for RF : number of trees etc
    
    - Thoughts
        - SalePrice tends to increase with LoanFrontage, MasVnrArea, BsmtFinSf1, TotalBsmtSF, 1stFlrSF, GrLivArea, GarageArea, WoodDeckSF, OpenPorchSF, 
        - not much affect with LotArea, 
        - No to negative affect with BsmtFinSf2, LowQualFinSF, EnclosedPorchSF, MiscVal
        - slight upward trend with BsmtUnfSF, 2stFlrSF, 3SsnPorch, ScreenPorch
        - sale price is higher in January and July
        
        Links:
            - Binning in pandas --> https://pbpython.com/pandas-qcut-cut.html
            - Correlation on categorical data --> https://stackoverflow.com/questions/47894387/how-to-correlate-an-ordinal-categorical-column-in-pandas

Attempt 4:
    - Do not delete the parameters that show negative trend with sale because they may be contributing to cheap houses
    - calculate corr() after converting columns to proper categories
    - Do we need to do one-hot encoding for random forest? 
      If yes, do we need to also take care of dummy variable trap for tree models? 
        ---> scikit learn takes features in numeric form so we need to do encoding; 
             we do not need to take care of dummy varaible trap for tree models /random forest
    - Regularization ; for random forest regularization is done by parameter tunning
    - Due to too many columns ; try catboost
    - Hyperparameter tunning for RF : number of trees etc
    
    - **Things that I researched/learned**:
        - Need to normalize numerical variables
        - may need to do log transformation
        - dummy encoding creates a sparse data and random forest performance will be worse
        - random forest favours features for split that gives more pure nodes, dummy encoded variables will not be favoured
        - H2O/Catboost dont require dummy encoding of categorical variables
        - Data Leakage 
        - Not going with this approach 
        - Most of my analysis done in third attempt is correct
        
    - LINKS:
        - DATSET pdf : http://jse.amstat.org/v19n3/decock.pdf
        - CATBOOST : https://catboost.ai
        - Alternates to One-HOT Encoding:https://github.com/scikit-learn-contrib/category_encoders
        - EDA : https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python
        - Model : https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset
        - Regularized Linear Model: https://www.kaggle.com/apapiu/regularized-linear-models
        - Stacked Model: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard
        - Data Leakage : https://www.kaggle.com/alexisbcook/data-leakage
        - Overfitting in Random Forest: https://stats.stackexchange.com/questions/111968/random-forest-how-to-handle-overfitting
        - Visual to RF: http://www.r2d3.us/visual-intro-to-machine-learning-part-1/
        - Categrical Variables and Tree models: https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931
        - Cat. v. Lost in RF: https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/
        - https://www.kaggle.com/learn/intermediate-machine-learning
        - scikit-learn Pipiline(?)
        - Great Resource: https://www.kaggle.com/learn/intermediate-machine-learning

Attempt 5 :
    - Do not combine test and train data --> train-test contamination 
    - Go through each column and put your analysis as per suggested EDA link above
    - Re-do EDA on data 
        - Univariate Analysis
            -Sale Price
                - boxplot | distplot
                - skewness and kurtosis
             - Rest of the columns 
                 - skewness and kurtosis
         - Bivariate Analysis
             - Sale price vs numerical  
                 - relplot
             - Sale price vs categorical
                 - box plot ; makes much sense to use boxplot compared to catplot and we get the same information in addition to outliers
        - Corelations
        - Missing Data 
    - Feature Engineering/Selection
        - convert columns to appropriate types
        - drop records based on analysis
        - Fill in missing values 
        Feature Engineering :
            - encode categories 
            - create new features 
            - binning
            - log transformations (does not help tree models because they ae scale invariant but will help linear model and NN)
        Feature Selection:
        - used lasso to select KBest Features
    - Applied similar transformations on test data
    - Simple Random Forest on 38 columns 
    - Third Kaggle Submission; score = 9.45943 --> Build a worst model 
    
    - Next: 
        - In EDA: chi sqaured correlations  or other point biserial etc.
        - In feature engineering :  - add squares of numerical columns 
        - univariate feature selection
        - Pipline for transformation
        - Do Regularized Regression
        - Do Random Forest on fewer columns
        - Do Catboost/H20 ; they do not require encoding of dummy variables
        - CatBoost Encoding of all categorical variables with na dropped and kept 

    - Links:
        - Skewness and Kurtosis: https://brownmath.com/stat/shape.htm
        - Probability Plot: https://blog.minitab.com/blog/applying-statistics-in-quality-projects/a-simple-guide-to-probability-plots   

What I learned: 
    - If skewness is less than âˆ’1 or greater than +1, the distribution is highly skewed.
    - A distribution with kurtosis >3 (excess kurtosis >0) is called leptokurtic. Compared to a normal distribution, its tails are longer and fatter, and often its central peak is higher and sharper.
    - If skwed data has 0 values; log tranformation wont work --> Add a low number for now, 
    later read this  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3444996
    - Dummifed data has no categorical data [Obvious]
    - standardscaler turns the dataframe into numpy arrays and then you losse column names. To keep the column information, use a different syntax
    - All things done for transformations on test data, must be part of a pipeline so we don't have to write similar code
    - Model spit out error because it sees NAN or inf value in test data for numerical columns;filled with 0 for now.  